{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOSELY SYMMETRIC NAIVE BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "import math\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_X_y, check_array, deprecated\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.utils.fixes import logsumexp\n",
    "from sklearn.utils.multiclass import _check_partial_fit_first_call\n",
    "from sklearn.utils.validation import check_is_fitted, check_non_negative, column_or_1d\n",
    "from sklearn.utils.validation import _check_sample_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailsDir = \"enron1/\"\n",
    "spamDir = os.path.join(mailsDir, \"spam\")\n",
    "hamDir = os.path.join(mailsDir, \"ham\")\n",
    "print(spamDir)\n",
    "print(hamDir)\n",
    "mails = []\n",
    "hams = []\n",
    "spams = []\n",
    "spaminfo = []\n",
    "\n",
    "i = 0\n",
    "hamDirList = os.listdir(hamDir)\n",
    "for file in hamDirList:\n",
    "    with open(os.path.join(hamDir, file), \"r\", encoding=\"latin-1\") as f:\n",
    "        mail = f.read()\n",
    "        mails.append(mail)\n",
    "        hams.append(mail)\n",
    "        spaminfo.append(0)\n",
    "        i += 1\n",
    "        if i == 100:\n",
    "            break\n",
    "            \n",
    "print(i)\n",
    "i = 0\n",
    "spamDirList = os.listdir(spamDir)\n",
    "for file in spamDirList:\n",
    "    with open(os.path.join(spamDir, file), \"r\", encoding=\"latin-1\") as f:\n",
    "        mail = f.read()\n",
    "        mails.append(mail)\n",
    "        spams.append(mail)\n",
    "        spaminfo.append(1)\n",
    "        i += 1\n",
    "        if i == 100:\n",
    "            break\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def tokenize(text, stemmer=porter_stemmer):\n",
    "    lower_text = text.lower()\n",
    "    tokens = nltk.wordpunct_tokenize(lower_text)\n",
    "    stems = [porter_stemmer.stem(token) for token in tokens]\n",
    "    punct_less = [stem for stem in stems if re.match(\n",
    "        '^[a-zA-Z]+$', stem\n",
    "    ) is not None]\n",
    "    return punct_less\n",
    "\n",
    "# stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "# with open(\"./stopwords.txt\", \"w\") as outf:\n",
    "#     outf.write(\"\\n\".join(stopwords))\n",
    "\n",
    "with open(\"./stopwords.txt\", \"r\") as inf:\n",
    "    stopwords = inf.read().splitlines()\n",
    "\n",
    "stop_words = []\n",
    "for word in stopwords:\n",
    "    stop_words.append(tokenize(word)[0])\n",
    "stop_words.append(\"becau\")\n",
    "stop_words = list(dict.fromkeys(stop_words))  # remove duplicates\n",
    "\n",
    "# binary=True \n",
    "# ngram_range=(2,2)\n",
    "\n",
    "# max_df slouzi k tomu, aby se nevyskytly 0 v deleni pri pocitani feature_log_prob\n",
    "# min_df je burstiness knockoff\n",
    "vec = CountVectorizer(\n",
    "    encoding=\"latin-1\",\n",
    "    decode_error=\"replace\",\n",
    "    strip_accents=\"unicode\",\n",
    "    analyzer=\"word\",\n",
    "    binary=False,\n",
    "    stop_words = stop_words,\n",
    "    tokenizer = tokenize,\n",
    "    ngram_range=(1,1),\n",
    "    max_df=0.99,\n",
    "    min_df=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2586, 9693)\n",
      "(2586, 9693)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mails, spaminfo, test_size = 0.5)\n",
    "\n",
    "# Count vectorizer\n",
    "X_train_count = vec.fit_transform(X_train)\n",
    "print(X_train_count.shape)\n",
    "\n",
    "# Count vectorizer\n",
    "X_test_count = vec.transform(X_test)\n",
    "print(X_test_count.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class _BaseNB(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):\n",
    "    \"\"\"Abstract base class for naive Bayes estimators\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Compute the unnormalized posterior log probability of X\n",
    "        I.e. ``log P(c) + log P(x|c)`` for all rows x of X, as an array-like of\n",
    "        shape [n_classes, n_samples].\n",
    "        Input is passed to _joint_log_likelihood as-is by predict,\n",
    "        predict_proba and predict_log_proba.\n",
    "        \"\"\"\n",
    "\n",
    "    def _check_X(self, X):\n",
    "        \"\"\"To be overridden in subclasses with the actual checks.\"\"\"\n",
    "        # Note that this is not marked @abstractmethod as long as the\n",
    "        # deprecated public alias sklearn.naive_bayes.BayesNB exists\n",
    "        # (until 0.24) to preserve backward compat for 3rd party projects\n",
    "        # with existing derived classes.\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform classification on an array of test vectors X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : ndarray of shape (n_samples,)\n",
    "            Predicted target values for X\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        X = self._check_X(X)\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        return self.classes_[np.argmax(jll, axis=1)]\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return log-probability estimates for the test vector X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like of shape (n_samples, n_classes)\n",
    "            Returns the log-probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute :term:`classes_`.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        X = self._check_X(X)\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        # normalize by P(x) = P(f_1, ..., f_n)\n",
    "        log_prob_x = logsumexp(jll, axis=1)\n",
    "        return jll - np.atleast_2d(log_prob_x).T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return probability estimates for the test vector X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like of shape (n_samples, n_classes)\n",
    "            Returns the probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute :term:`classes_`.\n",
    "        \"\"\"\n",
    "        return np.exp(self.predict_log_proba(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "_ALPHA_MIN = 1e-10\n",
    "\n",
    "class _BaseDiscreteNB(_BaseNB):\n",
    "    \"\"\"Abstract base class for naive Bayes on discrete/categorical data\n",
    "    Any estimator based on this class should provide:\n",
    "    __init__\n",
    "    _joint_log_likelihood(X) as per _BaseNB\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_X(self, X):\n",
    "        return check_array(X, accept_sparse='csr')\n",
    "\n",
    "    def _check_X_y(self, X, y):\n",
    "        return check_X_y(X, y, accept_sparse='csr')\n",
    "\n",
    "    def _update_class_log_prior(self, class_prior=None):\n",
    "        n_classes = len(self.classes_)\n",
    "        if class_prior is not None:\n",
    "            if len(class_prior) != n_classes:\n",
    "                raise ValueError(\"Number of priors must match number of\"\n",
    "                                 \" classes.\")\n",
    "            self.class_log_prior_ = np.log(class_prior)\n",
    "  \n",
    "        elif self.fit_prior:\n",
    "            with warnings.catch_warnings():\n",
    "                # silence the warning when count is 0 because class was not yet\n",
    "                # observed\n",
    "                warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "                log_class_count = np.log(self.class_count_)\n",
    "\n",
    "            # empirical prior, with sample_weight taken into account\n",
    "            self.class_log_prior_ = (log_class_count -\n",
    "                                     np.log(self.class_count_.sum()))\n",
    "            \n",
    "        else:\n",
    "            self.class_log_prior_ = np.full(n_classes, -np.log(n_classes))\n",
    "\n",
    "    def _check_alpha(self):\n",
    "        if np.min(self.alpha) < 0:\n",
    "            raise ValueError('Smoothing parameter alpha = %.1e. '\n",
    "                             'alpha should be > 0.' % np.min(self.alpha))\n",
    "        if isinstance(self.alpha, np.ndarray):\n",
    "            if not self.alpha.shape[0] == self.n_features_:\n",
    "                raise ValueError(\"alpha should be a scalar or a numpy array \"\n",
    "                                 \"with shape [n_features]\")\n",
    "        if np.min(self.alpha) < _ALPHA_MIN:\n",
    "            warnings.warn('alpha too small will result in numeric errors, '\n",
    "                          'setting alpha = %.1e' % _ALPHA_MIN)\n",
    "            return np.maximum(self.alpha, _ALPHA_MIN)\n",
    "        return self.alpha\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        X, y = self._check_X_y(X, y)\n",
    "        _, n_features = X.shape\n",
    "        self.n_features_ = n_features\n",
    "\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
    "        # We convert it to np.float64 to support sample_weight consistently;\n",
    "        # this means we also don't have to cast X to floating point\n",
    "        if sample_weight is not None:\n",
    "            Y = Y.astype(np.float64, copy=False)\n",
    "            sample_weight = np.asarray(sample_weight)\n",
    "            sample_weight = np.atleast_2d(sample_weight)\n",
    "            Y *= check_array(sample_weight).T\n",
    "\n",
    "        class_prior = self.class_prior\n",
    "\n",
    "        # Count raw events from data before updating the class log prior\n",
    "        # and feature log probas\n",
    "        n_effective_classes = Y.shape[1]\n",
    "\n",
    "        self._init_counters(n_effective_classes, n_features)\n",
    "        # added y here as a parameter, to be used in LSNB implementation of _count()\n",
    "        self._count(X, Y, y)\n",
    "        alpha = self._check_alpha()\n",
    "        self._update_feature_log_prob(alpha)\n",
    "        self._update_class_log_prior(class_prior=class_prior)\n",
    "        return self\n",
    "\n",
    "    def _init_counters(self, n_effective_classes, n_features):\n",
    "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "        self.feature_count_ = np.zeros((n_effective_classes, n_features),\n",
    "                                       dtype=np.float64)\n",
    "\n",
    "    # XXX The following is a stopgap measure; we need to set the dimensions\n",
    "    # of class_log_prior_ and feature_log_prob_ correctly.\n",
    "    def _get_coef(self):\n",
    "        return (self.feature_log_prob_[1:]\n",
    "                if len(self.classes_) == 2 else self.feature_log_prob_)\n",
    "\n",
    "    def _get_intercept(self):\n",
    "        return (self.class_log_prior_[1:]\n",
    "                if len(self.classes_) == 2 else self.class_log_prior_)\n",
    "\n",
    "    coef_ = property(_get_coef)\n",
    "    intercept_ = property(_get_intercept)\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {'poor_score': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class LooselySymmetricNB(_BaseDiscreteNB):\n",
    "    \n",
    "    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None, enhance=False):\n",
    "        self.alpha = alpha\n",
    "        self.fit_prior = fit_prior\n",
    "        self.class_prior = class_prior\n",
    "        self.enhance = enhance\n",
    "        \n",
    "    def _count(self, X, Y, y):\n",
    "        \"\"\"Count and smooth feature occurrences.\"\"\"\n",
    "        \n",
    "        check_non_negative(X, \"LooselySymmetricNB (input X)\")\n",
    "        self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
    "        self.class_count_ += Y.sum(axis=0)\n",
    "        \n",
    "        # we need these two values in order to calculate document frequency in _calculate_df()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def _update_feature_log_prob(self, alpha):\n",
    "        ### VLIV PARAMETRU ALPHA?\n",
    "        self.smoothed_fc = self.feature_count_ + alpha\n",
    "        self.smoothed_cc = self.smoothed_fc.sum(axis=1)\n",
    "        # funguje i radek nize?\n",
    "#         self.smoothed_cc = self.feature_count_.sum(axis=1)\n",
    "        \n",
    "        self._calculate_df()\n",
    "        self._calculate_abcd(self.smoothed_fc, self.smoothed_cc.reshape(-1, 1), self.enhance)\n",
    "        \n",
    "        # HAM\n",
    "        self.bd = (self.b * self.d) / (self.b + self.d)\n",
    "        self.ac = (self.a * self.c) / (self.a + self.c)\n",
    "        bd = (self.b * self.d) / (self.b + self.d)\n",
    "        ac = (self.a * self.c) / (self.a + self.c)\n",
    "        numerator = self.a + bd\n",
    "        denumerator = self.a + self.b + ac + bd\n",
    "        \n",
    "        # 2d pole, index 0 je pole ham, index 1 je pole spam\n",
    "        self.feature_log_prob_ = np.empty(self.feature_count_.shape) \n",
    "#         self.feature_log_prob_[0] = numerator / denumerator\n",
    "        self.feature_log_prob_[0] = np.log(numerator) - np.log(denumerator)\n",
    "        \n",
    "        # SPAM\n",
    "        numerator = self.c + bd\n",
    "        denumerator = self.c + self.d + ac + bd\n",
    "        \n",
    "#         self.feature_log_prob_[1] = numerator / denumerator\n",
    "        self.feature_log_prob_[1] = np.log(numerator) - np.log(denumerator)\n",
    "    \n",
    "    def _calculate_df(self):\n",
    "        \n",
    "        self.df = np.zeros(self.feature_count_.shape, dtype=np.int32)\n",
    "        for mail_idx, mail in enumerate(self.X.toarray()):\n",
    "            for word_idx, word in enumerate(mail):\n",
    "                if word >= 1:\n",
    "                    self.df[self.y[mail_idx]][word_idx] += 1\n",
    "    \n",
    "    def _calculate_abcd(self, fc, cc, enhance):\n",
    "        \n",
    "        # at 0 is ham info, at 1 is spam info\n",
    "        if enhance:\n",
    "            word_density_ham = fc[0] / cc[0]\n",
    "            word_density_spam = fc[1] / cc[1]\n",
    "        \n",
    "        else:\n",
    "            word_density_ham = 1\n",
    "            word_density_spam = 1\n",
    "        \n",
    "        self.a = (self.df[0] / self.class_count_[0]) * word_density_ham\n",
    "        self.b = (1 - self.a) * word_density_spam\n",
    "        self.c = (self.df[1] / self.class_count_[1]) * word_density_spam\n",
    "        self.d = (1 - self.c) * word_density_ham\n",
    "        \n",
    "        \n",
    "    def _joint_log_likelihood(self, X):\n",
    "       \n",
    "        return (safe_sparse_dot(X, self.feature_log_prob_.T) + \n",
    "                self.class_log_prior_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enron6/spam\n",
      "enron6/ham\n",
      "loaded\n"
     ]
    }
   ],
   "source": [
    "mailsDir = \"enron6/\"\n",
    "spamDir = os.path.join(mailsDir, \"spam\")\n",
    "hamDir = os.path.join(mailsDir, \"ham\")\n",
    "print(spamDir)\n",
    "print(hamDir)\n",
    "mails = []\n",
    "# hams = []\n",
    "# spams = []\n",
    "spaminfo = []\n",
    "\n",
    "hamDirList = os.listdir(hamDir)\n",
    "for file in hamDirList:\n",
    "    with open(os.path.join(hamDir, file), \"r\", encoding=\"latin-1\") as f:\n",
    "        mail = f.read()\n",
    "        mails.append(mail)\n",
    "#         hams.append(mail)\n",
    "        spaminfo.append(0)\n",
    "\n",
    "spamDirList = os.listdir(spamDir)\n",
    "for file in spamDirList:\n",
    "    with open(os.path.join(spamDir, file), \"r\", encoding=\"latin-1\") as f:\n",
    "        mail = f.read()\n",
    "        mails.append(mail)\n",
    "#         spams.append(mail)\n",
    "        spaminfo.append(1)\n",
    "\n",
    "ordered = list(zip(mails, spaminfo))\n",
    "random.shuffle(ordered)\n",
    "mails, spaminfo = zip(*ordered)\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "0.84\n",
      "0.875\n",
      "0.9545454545454546\n",
      "0.9130434782608695\n",
      "-----------\n",
      "100\n",
      "0.76\n",
      "0.8043478260869565\n",
      "0.925\n",
      "0.8604651162790697\n",
      "-----------\n",
      "200\n",
      "0.77\n",
      "0.8152173913043478\n",
      "0.9259259259259259\n",
      "0.8670520231213872\n",
      "-----------\n",
      "300\n",
      "0.8066666666666666\n",
      "0.8169014084507042\n",
      "0.9747899159663865\n",
      "0.888888888888889\n",
      "-----------\n",
      "400\n",
      "0.905\n",
      "0.8983050847457628\n",
      "0.99375\n",
      "0.9436201780415431\n",
      "-----------\n",
      "500\n",
      "0.82\n",
      "0.8185840707964602\n",
      "0.9788359788359788\n",
      "0.8915662650602411\n",
      "-----------\n",
      "600\n",
      "0.84\n",
      "0.8484848484848485\n",
      "0.9655172413793104\n",
      "0.9032258064516129\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "vec.binary = True\n",
    "\n",
    "for size in [50, 100, 200, 300, 400, 500, 600]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(mails[:size], spaminfo[:size], test_size = 0.5)\n",
    "    \n",
    "    # Count vectorizer\n",
    "    X_train_count = vec.fit_transform(X_train)\n",
    "    print(f\"{X_train_count.shape[0] *2}\")\n",
    "    spam_size_train = sum(y_train) / len(y_train)\n",
    "#     print(f\"spam size train: {spam_size_train}\")\n",
    "\n",
    "    # Count vectorizer\n",
    "    X_test_count = vec.transform(X_test)\n",
    "#     print(f\"X_test_count.shape: {X_test_count.shape}\")\n",
    "    spam_size_test = sum(y_test) / len(y_test)\n",
    "#     print(f\"spam size test: {spam_size_test}\")\n",
    "\n",
    "    bnb = BernoulliNB(class_prior=[0.5, 0.5])\n",
    "    bnb.fit(X_train_count, y_train)\n",
    "\n",
    "    print(f\"{bnb.score(X_test_count.toarray(), y_test)}\")\n",
    "    print(f\"{precision_score(y_test, bnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"{recall_score(y_test, bnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"{f1_score(y_test, bnb.predict(X_test_count.toarray()))}\")\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB, MultinomialNB, LSNB, eLSNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSNB\n",
      "50\n",
      "0.88\n",
      "0.8\n",
      "0.8888888888888888\n",
      "0.8421052631578948\n",
      "-----------\n",
      "ENHA\n",
      "50\n",
      "0.88\n",
      "0.8\n",
      "0.8888888888888888\n",
      "0.8421052631578948\n",
      "-----------\n",
      "GAUSSIAN\n",
      "50\n",
      "0.72\n",
      "0.75\n",
      "0.3333333333333333\n",
      "0.46153846153846156\n",
      "-----------\n",
      "MNB\n",
      "50\n",
      "0.84\n",
      "0.7272727272727273\n",
      "0.8888888888888888\n",
      "0.7999999999999999\n",
      "==============\n",
      "==============\n",
      "LSNB\n",
      "100\n",
      "0.9\n",
      "0.8076923076923077\n",
      "1.0\n",
      "0.8936170212765957\n",
      "-----------\n",
      "ENHA\n",
      "100\n",
      "0.92\n",
      "0.84\n",
      "1.0\n",
      "0.9130434782608696\n",
      "-----------\n",
      "GAUSSIAN\n",
      "100\n",
      "0.7\n",
      "0.8\n",
      "0.38095238095238093\n",
      "0.5161290322580645\n",
      "-----------\n",
      "MNB\n",
      "100\n",
      "0.9\n",
      "0.8636363636363636\n",
      "0.9047619047619048\n",
      "0.8837209302325582\n",
      "==============\n",
      "==============\n",
      "LSNB\n",
      "200\n",
      "0.83\n",
      "0.6875\n",
      "0.9428571428571428\n",
      "0.7951807228915663\n",
      "-----------\n",
      "ENHA\n",
      "200\n",
      "0.89\n",
      "0.8\n",
      "0.9142857142857143\n",
      "0.8533333333333333\n",
      "-----------\n",
      "GAUSSIAN\n",
      "200\n",
      "0.89\n",
      "0.9\n",
      "0.7714285714285715\n",
      "0.8307692307692307\n",
      "-----------\n",
      "MNB\n",
      "200\n",
      "0.9\n",
      "0.8205128205128205\n",
      "0.9142857142857143\n",
      "0.8648648648648648\n",
      "==============\n",
      "==============\n",
      "LSNB\n",
      "300\n",
      "0.8933333333333333\n",
      "0.7540983606557377\n",
      "0.9787234042553191\n",
      "0.8518518518518519\n",
      "-----------\n",
      "ENHA\n",
      "300\n",
      "0.8866666666666667\n",
      "0.734375\n",
      "1.0\n",
      "0.8468468468468469\n",
      "-----------\n",
      "GAUSSIAN\n",
      "300\n",
      "0.8733333333333333\n",
      "0.8043478260869565\n",
      "0.7872340425531915\n",
      "0.795698924731183\n",
      "-----------\n",
      "MNB\n",
      "300\n",
      "0.8866666666666667\n",
      "0.734375\n",
      "1.0\n",
      "0.8468468468468469\n",
      "==============\n",
      "==============\n",
      "LSNB\n",
      "400\n",
      "0.9\n",
      "0.7901234567901234\n",
      "0.9552238805970149\n",
      "0.8648648648648647\n",
      "-----------\n",
      "ENHA\n",
      "400\n",
      "0.94\n",
      "0.8767123287671232\n",
      "0.9552238805970149\n",
      "0.9142857142857143\n",
      "-----------\n",
      "GAUSSIAN\n",
      "400\n",
      "0.91\n",
      "0.9298245614035088\n",
      "0.7910447761194029\n",
      "0.8548387096774193\n",
      "-----------\n",
      "MNB\n",
      "400\n",
      "0.94\n",
      "0.8767123287671232\n",
      "0.9552238805970149\n",
      "0.9142857142857143\n",
      "==============\n",
      "==============\n",
      "LSNB\n",
      "500\n",
      "0.928\n",
      "0.8514851485148515\n",
      "0.9662921348314607\n",
      "0.9052631578947369\n",
      "-----------\n",
      "ENHA\n",
      "500\n",
      "0.948\n",
      "0.9042553191489362\n",
      "0.9550561797752809\n",
      "0.9289617486338798\n",
      "-----------\n",
      "GAUSSIAN\n",
      "500\n",
      "0.884\n",
      "0.9285714285714286\n",
      "0.7303370786516854\n",
      "0.8176100628930818\n",
      "-----------\n",
      "MNB\n",
      "500\n",
      "0.948\n",
      "0.9042553191489362\n",
      "0.9550561797752809\n",
      "0.9289617486338798\n",
      "==============\n",
      "==============\n",
      "LSNB\n",
      "600\n",
      "0.9166666666666666\n",
      "0.7798165137614679\n",
      "0.9883720930232558\n",
      "0.8717948717948718\n",
      "-----------\n",
      "ENHA\n",
      "600\n",
      "0.9433333333333334\n",
      "0.8556701030927835\n",
      "0.9651162790697675\n",
      "0.907103825136612\n",
      "-----------\n",
      "GAUSSIAN\n",
      "600\n",
      "0.9533333333333334\n",
      "0.9285714285714286\n",
      "0.9069767441860465\n",
      "0.9176470588235294\n",
      "-----------\n",
      "MNB\n",
      "600\n",
      "0.9466666666666667\n",
      "0.8645833333333334\n",
      "0.9651162790697675\n",
      "0.9120879120879122\n",
      "==============\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "vec.binary = False\n",
    "\n",
    "for size in [50, 100, 200, 300, 400, 500, 600]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(mails[:size], spaminfo[:size], test_size = 0.5)\n",
    "    \n",
    "    # Count vectorizer\n",
    "    X_train_count = vec.fit_transform(X_train)\n",
    "#     print(f\"X_train_count.shape: {X_train_count.shape[0]}\")\n",
    "#     print(f\"{X_train_count.shape[0] * 2}\")\n",
    "    spam_size_train = sum(y_train) / len(y_train)\n",
    "#     print(f\"spam size train: {spam_size_train}\")\n",
    "\n",
    "    # Count vectorizer\n",
    "    X_test_count = vec.transform(X_test)\n",
    "#     print(f\"X_test_count.shape: {X_test_count.shape}\")\n",
    "    spam_size_test = sum(y_test) / len(y_test)\n",
    "#     print(f\"spam size test: {spam_size_test}\")\n",
    "    \n",
    "    gnb = GaussianNB(priors=[0.5, 0.5])\n",
    "    gnb.fit(X_train_count.toarray(), y_train)\n",
    "    mnb = MultinomialNB(class_prior=[0.5, 0.5])\n",
    "    mnb.fit(X_train_count, y_train)\n",
    "    lsnb = LooselySymmetricNB(class_prior=[0.5, 0.5])\n",
    "    lsnb.fit(X_train_count, y_train)\n",
    "    elsnb = LooselySymmetricNB(class_prior=[0.5, 0.5], enhance=True)\n",
    "    elsnb.fit(X_train_count, y_train)\n",
    "\n",
    "    \n",
    "    print(\"LSNB\")\n",
    "    print(f\"{X_train_count.shape[0] * 2}\")\n",
    "    print(f\"{lsnb.score(X_test_count.toarray(), y_test)}\")\n",
    "    print(f\"{precision_score(y_test, lsnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"{recall_score(y_test, lsnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"{f1_score(y_test, lsnb.predict(X_test_count.toarray()))}\")\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    print(\"ENHA\")\n",
    "    print(f\"{X_train_count.shape[0] * 2}\")\n",
    "    print(f\"{elsnb.score(X_test_count.toarray(), y_test)}\")\n",
    "    print(f\"{precision_score(y_test, elsnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"{recall_score(y_test, elsnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"{f1_score(y_test, elsnb.predict(X_test_count.toarray()))}\")\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    print(\"GAUSSIAN\")\n",
    "    print(f\"{X_train_count.shape[0] * 2}\")\n",
    "    print(f\"{gnb.score(X_test_count.toarray(), y_test)}\")\n",
    "    print(f\"{precision_score(y_test, gnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"{recall_score(y_test, gnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"{f1_score(y_test, gnb.predict(X_test_count.toarray()))}\")\n",
    "    print(\"-----------\")\n",
    "\n",
    "    print(\"MNB\")\n",
    "    print(f\"{X_train_count.shape[0] * 2}\")\n",
    "    print(f\"{mnb.score(X_test_count.toarray(), y_test)}\")\n",
    "    print(f\"{precision_score(y_test, mnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"{recall_score(y_test, mnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"{f1_score(y_test, mnb.predict(X_test_count.toarray()))}\")\n",
    "    print(\"==============\")\n",
    "    print(\"==============\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
